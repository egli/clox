* Chunks of Bytecode
** TODO Run-length encoding for lines
Our encoding of line information is hilariously wasteful of memory.
Given that a series of instructions often correspond to the same
source line, a natural solution is something akin to run-length
encoding of the line numbers.

Devise an encoding that compresses the line information for a series
of instructions on the same line. Change writeChunk() to write this
compressed form, and implement a getLine() function that, given the
index of an instruction, determines the line where the instruction
occurs.

Hint: It’s not necessary for getLine() to be particularly efficient.
Since it is called only when a runtime error occurs, it is well off
the critical path where performance matters.

** DONE OP_CONSTANT_LONG
CLOSED: [2025-08-28 Do 16:15]
Because ~OP_CONSTANT~ uses only a single byte for its operand, a chunk
may only contain up to 256 different constants. That’s small enough
that people writing real-world code will hit that limit. We could use
two or more bytes to store the operand, but that makes every constant
instruction take up more space. Most chunks won’t need that many
unique constants, so that wastes space and sacrifices some locality in
the common case to support the rare case.

To balance those two competing aims, many instruction sets feature
multiple instructions that perform the same operation but with
operands of different sizes. Leave our existing one-byte ~OP_CONSTANT~
instruction alone, and define a second ~OP_CONSTANT_LONG~ instruction.
It stores the operand as a 24-bit number, which should be plenty.

Implement this function:

#+begin_src lox
void writeConstant(Chunk* chunk, Value value, int line) {
  // Implement me...
}
#+end_src

It adds value to chunk’s constant array and then writes an appropriate
instruction to load the constant. Also add support to the disassembler
for ~OP_CONSTANT_LONG~ instructions.

Defining two instructions seems to be the best of both worlds. What
sacrifices, if any, does it force on us?

See the exercise/14.2 branch for an implementation.

** TODO DIY malloc() and free()
Our ~reallocate()~ function relies on the C standard library for dynamic
memory allocation and freeing. ~malloc()~ and ~free()~ aren’t magic. Find
a couple of open source implementations of them and explain how they
work. How do they keep track of which bytes are allocated and which
are free? What is required to allocate a block of memory? Free it? How
do they make that efficient? What do they do about fragmentation?

Hardcore mode: Implement ~reallocate()~ without calling ~realloc()~,
~malloc()~, or ~free()~. You are allowed to call ~malloc()~ once, at
the beginning of the interpreter’s execution, to allocate a single big
block of memory, which your ~reallocate()~ function has access to. It
parcels out blobs of memory from that single region, your own personal
heap. It’s your job to define how it does that.

* A Virtual Machine
** DONE Bytecode instruction sequences
CLOSED: [2025-09-10 Mi 18:39]
What bytecode instruction sequences would you generate for the following expressions:

#+begin_example
1 * 2 + 3
1 + 2 * 3
3 - 2 - 1
1 + 2 * 3 - 4 / -5
#+end_example

(Remember that Lox does not have a syntax for negative number
literals, so the -5 is negating the number 5.)

#+begin_example
;; 1 * 2 + 3
OP_CONSTANT 1
OP_CONSTANT 2
OP_MULTIPLY
OP_CONSTANT 3
OP_ADD
;; 1 + 2 * 3
OP_CONSTANT 1
OP_CONSTANT 2
OP_CONSTANT 3
OP_MULTIPLY
OP_ADD
;; 3 - 2 - 1
OP_CONSTANT 3
OP_CONSTANT 2
OP_SUBTRACT
OP_CONSTANT 1
OP_SUBTRACT
;; 1 + 2 * 3 - 4 / -5
OP_CONSTANT 1
OP_CONSTANT 2
OP_CONSTANT 3
OP_MULTIPLY
OP_ADD
OP_CONSTANT 4
OP_CONSTANT 5
OP_NEGATE
OP_DIVIDE
OP_SUBTRACT
#+end_example

** TODO Eliminate ~OP_NEGATE~
If we really wanted a minimal instruction set, we could eliminate
either ~OP_NEGATE~ or ~OP_SUBTRACT~. Show the bytecode instruction
sequence you would generate for:

First, without using ~OP_NEGATE~. Then, without using ~OP_SUBTRACT~.

Given the above, do you think it makes sense to have both
instructions? Why or why not? Are there any other redundant
instructions you would consider including?

#+begin_example
4 - 3 * -2
#+end_example

#+begin_example
;; 4 - 3 * -2
OP_CONSTANT 4
OP_CONSTANT 5
OP_CONSTANT 2
OP_NEGATE
OP_MULTIPLY
OP_SUBTRACT
#+end_example

Convert -2 to 0-2

#+begin_example
;; 4 - 3 * -2
OP_CONSTANT 4
OP_CONSTANT 5
OP_CONSTANT 0
OP_CONSTANT 2
OP_SUBTRACT
OP_MULTIPLY
OP_SUBTRACT
#+end_example

Convert a-b to a+-b
#+begin_example
;; 4 - 3 * -2
OP_CONSTANT 4
OP_CONSTANT 5
OP_CONSTANT 2
OP_NEGATE
OP_MULTIPLY
OP_NEGATE
OP_ADD
#+end_example

** TODO Dynamic stack
Our VM’s stack has a fixed size, and we don’t check if pushing a value
overflows it. This means the wrong series of instructions could cause
our interpreter to crash or go into undefined behavior. Avoid that by
dynamically growing the stack as needed.

What are the costs and benefits of doing so?

** TODO ~OP_NEGATE~ in place
To interpret ~OP_NEGATE~, we pop the operand, negate the value, and
then push the result. That’s a simple implementation, but it
increments and decrements ~stackTop~ unnecessarily, since the stack
ends up the same height in the end. It might be faster to simply
negate the value in place on the stack and leave ~stackTop~ alone. Try
that and see if you can measure a performance difference.

Are there other instructions where you can do a similar optimization?

* Scanning on Demand
** TODO String interpolation
Many newer languages support string interpolation. Inside a string
literal, you have some sort of special delimiters—most commonly ${ at
the beginning and } at the end. Between those delimiters, any
expression can appear. When the string literal is executed, the inner
expression is evaluated, converted to a string, and then merged with
the surrounding string literal.

For example, if Lox supported string interpolation, then this...

#+begin_src lox
var drink = "Tea";
var steep = 4;
var cool = 2;
print "${drink} will be ready in ${steep + cool} minutes.";
#+end_src

... would print:

#+begin_example
Tea will be ready in 6 minutes.
#+end_example

What token types would you define to implement a scanner for string
interpolation? What sequence of tokens would you emit for the above
string literal?

What tokens would you emit for:

#+begin_example
"Nested ${"interpolation?! Are you ${"mad?!"}"}"
#+end_example

Consider looking at other language implementations that support
interpolation to see how they handle it.

** DONE '>>' vs '>'
CLOSED: [2025-09-25 Do 14:07]
Several languages use angle brackets for generics and also have a ~>>~
right shift operator. This led to a classic problem in early versions
of C++:

#+begin_src cpp
vector<vector<string>> nestedVectors;
#+end_src

This would produce a compile error because the ~>>~ was lexed to a
single right shift token, not two ~>~ tokens. Users were forced to
avoid this by putting a space between the closing angle brackets.

Later versions of C++ are smarter and can handle the above code. Java
and C# never had the problem. How do those languages specify and
implement this?

*** Answer
The C# and the Java lexers follow the maximal munch" rule, i.e. eat as
much as they can. However they avoid the ~>>~ problem by being aware
of the context in this case the closing generics. In that case the
~>>~ are interpreted as two consecutive '>' tokens.

Implementation approach:
- Track generic nesting depth in parser
- When seeing '>', check if in generic context
- If so, treat each '>' separately for closing generics
- Otherwise, combine as '>>' for right-shift operator

** DONE Contextual Keywords
CLOSED: [2025-09-25 Do 13:59]

Many languages, especially later in their evolution, define
“contextual keywords”. These are identifiers that act like reserved
words in some contexts but can be normal user-defined identifiers in
others.

For example, ~await~ is a keyword inside an ~async~ method in C#, but
in other methods, you can use ~await~ as your own identifier.

Name a few contextual keywords from other languages, and the context
where they are meaningful. What are the pros and cons of having
contextual keywords? How would you implement them in your language’s
front end if you needed to?

*** Answer
- [X] Name a few
  - Apparently Java uses contextual keywords in module declarations
- [X] Pros and cons
  + backward compatibility (existing code using these words as identifiers won't break)
  + Language evolution without breaking changes
  + Increased parser complexity
  + Can be confusing for developers (same word has different meanings)
  + Makes language specification more complex
  + Can make code harder to read if the context isn't immediately clear
- [X] Implementation?
  1. Lexer: Tokenize these words as regular identifiers
  2. Parser: Check the parsing context (e.g., inside a property
     declaration)
  3. When in the relevant context, check if the identifier matches a
     contextual keyword
  4. If matched, treat it as the special syntax; otherwise, process as
     regular identifier

* Compiling Expressions
** DONE Call trace
CLOSED: [2025-10-10 Fr 17:15]
To really understand the parser, you need to see how execution threads
through the interesting parsing functions -- ~parsePrecedence()~ and
the parser functions stored in the table. Take this (strange)
expression:

#+begin_example
(-1 + 2) * 3 - -4
#+end_example

Write a trace of how those functions are called. Show the order they
are called, which calls which, and the arguments passed to them.

*** Answer
compile()
├── expression()
    ├── parsePrecedence(PREC_ASSIGNMENT)
        ├── grouping()  // for "(-1 + 2)"
        │   ├── expression()
        │       ├── parsePrecedence(PREC_ASSIGNMENT)
        │           ├── unary()  // for "-1"
        │           │   ├── parsePrecedence(PREC_UNARY)
        │           │       └── number()  // for "1"
        │           └── binary()  // for "+"
        │               ├── parsePrecedence(PREC_FACTOR)  // PREC_TERM + 1
        │                   └── number()  // for "2"
        └── binary()  // for "*"
        │   ├── parsePrecedence(PREC_UNARY)  // PREC_FACTOR + 1
        │       └── number()  // for "3"
        └── binary()  // for "-"
            ├── parsePrecedence(PREC_FACTOR)  // PREC_TERM + 1
                ├── unary()  // for "-4"
                    ├── parsePrecedence(PREC_UNARY)
                        └── number()  // for "4"

For (-1 + 2) - 3 * -4:

compile()
├── expression()
    ├── parsePrecedence(PREC_ASSIGNMENT)
        ├── grouping()  // for "(-1 + 2)"
        │   ├── expression()
        │       ├── parsePrecedence(PREC_ASSIGNMENT)
        │           ├── unary()  // for "-1"
        │           │   ├── parsePrecedence(PREC_UNARY)
        │           │       └── number()  // for "1"
        │           └── binary()  // for "+"
        │               ├── parsePrecedence(PREC_FACTOR)  // PREC_TERM + 1
        │                   └── number()  // for "2"
        └── binary()  // for "-"
            ├── parsePrecedence(PREC_FACTOR)  // PREC_TERM + 1
                ├── number()  // for "3"
                └── binary()  // for "*"
                    ├── parsePrecedence(PREC_UNARY)  // PREC_FACTOR + 1
                        ├── unary()  // for "-4"
                            ├── parsePrecedence(PREC_UNARY)
                                └── number()  // for "4"

** DONE Both prefix and infix
CLOSED: [2025-10-10 Fr 17:15]
The ParseRule row for ~TOKEN_MINUS~ has both prefix and infix function
pointers. That’s because ~-~ is both a prefix operator (unary
negation) and an infix one (subtraction).

In the full Lox language, what other tokens can be used in both prefix
and infix positions? What about in C or in another language of your
choice?

*** Answer
As far as I know the only token that is both prefix and infix is ~-~.

- C :: additionally has ~*~ (arithmetic, dereferencing), ~&~
  (address-of, bitwise AND)
- Python :: has ~*~ for unpacking
- Rust :: has ~*~ for dereferencing and ~&~ for reference/borrow

** DONE "Mixfix" expressions
CLOSED: [2025-10-10 Fr 17:54]
You might be wondering about complex "mixfix" expressions that have
more than two operands separated by tokens. C’s conditional or
“ternary” operator, ~?:~, is a widely known one.

Add support for that operator to the compiler. You don’t have to
generate any bytecode, just show how you would hook it up to the
parser and handle the operands.

*** Answer
See branch exercise/17.3

You need to make the following changes:

**** Add tokens to the scanner
#+begin_src c
TOKEN_QUESTION,    // ?
TOKEN_COLON,       // :
#+end_src

**** Add precedence level
#+begin_src c
typedef enum {
  PREC_NONE,
  PREC_ASSIGNMENT, // =
  PREC_TERNARY,    // ? :  (new - just above assignment)
  PREC_OR,         // or
  // ... rest unchanged
} Precedence;
#+end_src

**** Add bytecode instruction
#+begin_src c
// In chunk.h or wherever opcodes are defined
OP_CONDITIONAL,  // Ternary conditional operator
#+end_src

**** Implement the parsing function
#+begin_src c
static void ternary() {
  // The condition is already compiled by the time we get here

  // Compile the "then" expression
  expression();
  consume(TOKEN_COLON, "Expect ':' after then branch of ternary operator.");

  // Compile the "else" expression
  parsePrecedence(PREC_TERNARY);

  emitByte(OP_CONDITIONAL);
}
#+end_src

The key insight is that the ternary operator is right-associative, so
when parsing the "else" part, we use =PREC_TERNARY= (not
=PREC_TERNARY + 1=) to allow chaining like =a ? b : c ? d : e=.

**** Update the parse rules table
#+begin_src c
ParseRule rules[] = {
  // ... existing rules ...
  [TOKEN_QUESTION]    = {NULL,     ternary, PREC_TERNARY},
  [TOKEN_COLON]       = {NULL,     NULL,    PREC_NONE},
  // ... rest unchanged
};
#+end_src

**** Implement the VM instruction
#+begin_src c
// In vm.c, inside the instruction switch
case OP_CONDITIONAL: {
  Value elseValue = pop();
  Value thenValue = pop();
  Value condition = pop();

  push(isFalsey(condition) ? elseValue : thenValue);
  break;
}
#+end_src

**** Implement the bytecode in the disassembler
#+begin_src c
    case OP_CONDITIONAL:
    return simpleInstruction("OP_CONDITIONAL", offset);
#+end_src

* Types of Values
** DONE Eliminate instructions
CLOSED: [2025-11-12 Mi 20:47]
We could reduce our binary operators even further than we did here.
Which other instructions can you eliminate, and how would the compiler
cope with their absence?

- ~OP_MULTIPLY~ in terms of ~OP_ADD~
- ~OP_NIL~, ~OP_TRUE~ and ~OP_FALSE~ in terms of ~OP_CONSTANT~ with
  some well crafted constants array
- ~OP_TRUE~ as ~OP_FALSE~ followed by ~OP_NOT~
- ~OP_SUBTRACT~ as ~OP_NEGATE~ followed by ~OP_ADD~ (?)


** DONE Adding instruction
CLOSED: [2025-11-12 Mi 20:47]
Conversely, we can improve the speed of our bytecode VM by adding more
specific instructions that correspond to higher-level operations. What
instructions would you define to speed up the kind of user code we
added support for in this chapter?

- specific constants, like 0 or 1, similarly to ~OP_TRUE~ we could
  have ~OP_ONE~
- specific constant arithmetic operations, i.e. + 1 could become ~OP_ADD_ONE~
- ah or adding a constant could become ~OP_ADD_CONST~

* Strings

** TODO Flexible array members
Each string requires two separate dynamic allocations—one for the
ObjString and a second for the character array. Accessing the
characters from a value requires two pointer indirections, which can
be bad for performance. A more efficient solution relies on a
technique called [[https://en.wikipedia.org/wiki/Flexible_array_member][flexible array members]]. Use that to store the
ObjString and its character array in a single contiguous allocation.

** TODO Support for constant strings
When we create the ObjString for each string literal, we copy the
characters onto the heap. That way, when the string is later freed, we
know it is safe to free the characters too.

This is a simpler approach but wastes some memory, which might be a
problem on very constrained devices. Instead, we could keep track of
which ObjStrings own their character array and which are “constant
strings” that just point back to the original source string or some
other non-freeable location. Add support for this.

** TODO Add string and other types
If Lox was your language, what would you have it do when a user tries
to use + with one string operand and the other some other type?
Justify your choice. What do other languages do?


